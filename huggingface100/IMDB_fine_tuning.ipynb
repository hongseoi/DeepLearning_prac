{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 003. IMDB\n",
    "- 영화 리뷰 긍/부정 판단 위한 sentiment analysis dataset\n",
    "- 25000 train data, 25000 test data로 구성\n",
    "- torxhtext를 이용해 데이터 다운로드하기\n",
    "\n",
    "``` Python\n",
    "! pip install torchtext=0.15.2 # 자연어 처리 작업\n",
    "! pip install portalocker=2.7.0 # 여러 프로세스가 동일한  파일을 동시에 접근할 때 충돌 방지\n",
    "! pip install accelerate -U # pytorch 모델의 학습 및 평가 가속화\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위까지 실행 후 restart\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_iter = IMDB(split='train')\n",
    "test_iter = IMDB(split='test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, \"I LOVED this movie! I am biased seeing as I am a huge Disney fan, but I really enjoyed myself. The action takes off running in the beginning of the film and just keeps going! This is a bit of a departure for Disney, they don't spend quite as much time on character development (my husband pointed this out)and there are no musical numbers. It is strictly action adventure. I thoroughly enjoyed it and recommend it to anyone who loves Disney, be they young or old.\")\n",
      "(1, 'This was an abysmal show. In short it was about this kid called Doug who guilt-tripped a lot. Seriously he could feel guilty over killing a fly then feeling guilty over feeling guilty for killing the fly and so forth. The animation was grating and unpleasant and the jokes cheap. <br /><br />It aired here in Sweden as a part of the \"Disney time\" show and i remember liking it some what but then i turned 13.<br /><br />I never got why some of the characters were green and purple too. What was up with that? <br /><br />Truly a horrible show. Appareantly it spawned a movie which i\\'ve never seen but i don\\'t have any great expectations for that one either.')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(6)\n",
    "\n",
    "# xxx_iter를 리스트로 변경\n",
    "train_list = list(train_iter)\n",
    "test_list = list(test_iter)\n",
    "\n",
    "# 리스트 중 1000개씩 랜덤샘플링\n",
    "train_list_small = random.sample(train_list, 1000)\n",
    "test_list_small = random.sample(test_list, 1000)\n",
    "\n",
    "# 각 변수의 첫번째 원소 출력\n",
    "print(train_list_small[0])\n",
    "print(test_list_small[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 005. label encoding\n",
    "위의 데이터셋은 (텍스트, 레이블) 형태로 구성되어 있으며, 레이블의 경우 2는 긍정, 1은 부정으로 구성되어 있다. 긍정을 1, 부정을 0으로 바꾸고 이걸 각각 train_texts, train_labels, test_texts, test_labels에 저장하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts  = []\n",
    "train_labels = []\n",
    "test_texts = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, text in train_list_small:\n",
    "    train_labels.append(1 if label==2 else 0)\n",
    "    train_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, text in test_list_small:\n",
    "    test_labels.append(1 if label==2 else 0)\n",
    "    test_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "I LOVED this movie! I am biased seeing as I am a huge Disney fan, but I really enjoyed myself. The action takes off running in the beginning of the film and just keeps going! This is a bit of a departure for Disney, they don't spend quite as much time on character development (my husband pointed this out)and there are no musical numbers. It is strictly action adventure. I thoroughly enjoyed it and recommend it to anyone who loves Disney, be they young or old.\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0])\n",
    "print(train_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 006. 학습 및 검증 데이터 분리\n",
    "1000개를 8:2로 나누어라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=3)\n",
    "\n",
    "print(len(train_labels))\n",
    "print(len(train_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 007. 토크나이징 및 인코딩\n",
    "위에서 추출한 train, val, test 데이터를 pretrained distilbert-base-uncased 모델에 투입하기 위해 토크나이저를 사용해서 인코딩하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.37.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import DistilBertTokenizerFast #DistilBertTokenizerfast\n",
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 4937, 11350, 2038, 2048]\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 실행\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True) # truncation: 모델의 default max_length를 넘는 입력 부분을 더 이상 받지 않고 절단함\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "\n",
    "print(train_encodings['input_ids'][0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 4937, 11350, 2038, 2048]\n",
      "[CLS] cat soup has two\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings['input_ids'][0][:5])\n",
    "print(tokenizer.decode(train_encodings['input_ids'][0][:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 008. 데이터세트 클래스 생성\n",
    "- torch.utils.data.Dataset을 상속하는 IMDBDataset이라는 클래스 작성하기\n",
    "- 문제 007의 imdb 데이터셋에서 학습한 train/val/test encodings를 입력해서 클래스 인스턴스화하기\n",
    "\n",
    "cf. __init__은 인스턴스화에 생기는 generator이다. generator는 클래스로 객체를 생성할 때 자동으로 호출되는 메서드로, 객체를 미리 설정한 값으로 초기화한다. 그리고 클래스를 구성하는 arguments, variable, function을 정의한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # {'key':'value'} 형태의 딕셔너리 구조\n",
    "        # val[idx], label[idx]의 데이터를 파이토치 텐서로 변환\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.IMDBDataset at 0x1ac7da133e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = IMDBDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDBDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDBDataset(test_encodings, test_labels)\n",
    "\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "# 각 데이터셋 확인: iterable 객체이므로 for문 사용가능\n",
    "for i in train_dataset:\n",
    "    print(i.keys())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input_ids: 단어 ID\n",
    "- attention_mask: 패딩 위치(실제 입력 토큰과 패딩 토큰 구분)\n",
    "- labels\n",
    "\n",
    "## 사전학습 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# 1. pretrained model 불러오기\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. training_argument 설정\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./result',\n",
    "    num_train_epochs = 8,\n",
    "    per_device_train_batch_size = 16, # 학습시 디바이스별 미니 배치 수\n",
    "    per_device_eval_batch_size = 64, # 평가시 ~\n",
    "    warmup_steps = 500, # 학습률 스케줄링용 웜업 스텝수\n",
    "    weight_decay=0.01, # 가중치 감소 강도\n",
    "    logging_dir = './logs',\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 모델을 GPU에 전송\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  4 17:09:18 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.42                 Driver Version: 537.42       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti   WDDM  | 00000000:07:00.0  On |                  N/A |\n",
      "|  0%   38C    P2              24W / 220W |   2018MiB /  8192MiB |     22%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      3472    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      4020    C+G   ...wekyb3d8bbwe\\XboxGameBarWidgets.exe    N/A      |\n",
      "|    0   N/A  N/A      6236      C   ...er\\anaconda3\\envs\\kaggle\\python.exe    N/A      |\n",
      "|    0   N/A  N/A      7832    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      9444    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9484    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9492    C+G   ...__8wekyb3d8bbwe\\Microsoft.Notes.exe    N/A      |\n",
      "|    0   N/A  N/A     13416    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13468    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     14004    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     14028    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     15072    C+G   ...US\\ArmouryDevice\\asus_framework.exe    N/A      |\n",
      "|    0   N/A  N/A     17296    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     18068    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     18656    C+G   ...al\\Discord\\app-1.0.9038\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     20416    C+G   ...a\\Local\\slack\\app-4.37.98\\slack.exe    N/A      |\n",
      "|    0   N/A  N/A     21036    C+G   ...256.0_x64__dt26b99r8h8gj\\RtkUWP.exe    N/A      |\n",
      "|    0   N/A  N/A     21424    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21460    C+G   ...on\\123.0.2420.65\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     22372    C+G   ...4.38_x64__8wekyb3d8bbwe\\msteams.exe    N/A      |\n",
      "|    0   N/A  N/A     22636    C+G   ...on\\122.0.2365.92\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     23420    C+G   ....0_x64__8wekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
      "|    0   N/A  N/A     23432    C+G   ...520_x64__8wekyb3d8bbwe\\ms-teams.exe    N/A      |\n",
      "|    0   N/A  N/A     24036    C+G   ...on\\122.0.2365.92\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     25564    C+G   ...ta\\Local\\Programs\\Notion\\Notion.exe    N/A      |\n",
      "|    0   N/A  N/A     29340    C+G   ...__8wekyb3d8bbwe\\Notepad\\Notepad.exe    N/A      |\n",
      "|    0   N/A  N/A     29616    C+G   ...esktop\\app-3.3.12\\GitHubDesktop.exe    N/A      |\n",
      "|    0   N/A  N/A     31116    C+G   ...e Stream\\89.0.2.0\\GoogleDriveFS.exe    N/A      |\n",
      "|    0   N/A  N/A     31704    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     32004    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "## 파인튜닝 이전 모델 극성 판별 평가\n",
    "texts = [\n",
    "    \"I feel fantastic\",\n",
    "    \"My life is going something wrong\",\n",
    "    \"I have not figured out what the chosen title has to do with the movie.\"\n",
    "]\n",
    "\n",
    "# 토크나이징\n",
    "input_tokens = tokenizer(texts, truncation=True, padding=True)\n",
    "print(input_tokens.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 토크나이징 결과를 모델에 투입하고 모델 출력 결과를 GPU에 전송\n",
    "outputs = model(torch.tensor(input_tokens['input_ids']).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'negative', 'negative']\n"
     ]
    }
   ],
   "source": [
    "# 레이블 딕셔너리 생성\n",
    "label_dict = {0:'positive', 1:'negative'}\n",
    "\n",
    "print([label_dict[i] for i in torch.argmax(outputs['logits'], axis=1).cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "  2%|▎         | 10/400 [00:08<05:25,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6915, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 20/400 [00:17<05:15,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6868, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 30/400 [00:25<05:05,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6891, 'learning_rate': 3e-06, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 40/400 [00:33<04:57,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.683, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 50/400 [00:41<04:49,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6831, 'learning_rate': 5e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 60/400 [00:50<04:40,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6701, 'learning_rate': 6e-06, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 70/400 [00:58<04:32,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6619, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 80/400 [01:06<04:25,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.63, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 90/400 [01:14<04:15,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5692, 'learning_rate': 9e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 100/400 [01:23<04:08,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4975, 'learning_rate': 1e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 110/400 [01:31<03:59,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4562, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 120/400 [01:39<03:51,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3588, 'learning_rate': 1.2e-05, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 130/400 [01:48<03:46,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3524, 'learning_rate': 1.3000000000000001e-05, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 140/400 [01:56<03:36,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3749, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 150/400 [02:04<03:30,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3253, 'learning_rate': 1.5e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 160/400 [02:13<03:19,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2744, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 170/400 [02:21<03:14,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2606, 'learning_rate': 1.7000000000000003e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 180/400 [02:30<03:04,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2421, 'learning_rate': 1.8e-05, 'epoch': 3.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 190/400 [02:38<02:54,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2188, 'learning_rate': 1.9e-05, 'epoch': 3.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 200/400 [02:46<02:46,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1909, 'learning_rate': 2e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 210/400 [02:55<02:40,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0819, 'learning_rate': 2.1e-05, 'epoch': 4.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 220/400 [03:03<02:31,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1242, 'learning_rate': 2.2000000000000003e-05, 'epoch': 4.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 230/400 [03:11<02:22,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0571, 'learning_rate': 2.3000000000000003e-05, 'epoch': 4.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 240/400 [03:20<02:14,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1319, 'learning_rate': 2.4e-05, 'epoch': 4.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 250/400 [03:28<02:04,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0911, 'learning_rate': 2.5e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 260/400 [03:36<01:55,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0392, 'learning_rate': 2.6000000000000002e-05, 'epoch': 5.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 270/400 [03:45<01:47,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0604, 'learning_rate': 2.7000000000000002e-05, 'epoch': 5.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 280/400 [03:53<01:39,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0375, 'learning_rate': 2.8000000000000003e-05, 'epoch': 5.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 290/400 [04:01<01:31,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0565, 'learning_rate': 2.9e-05, 'epoch': 5.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 300/400 [04:10<01:22,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0921, 'learning_rate': 3e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 310/400 [04:18<01:14,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0704, 'learning_rate': 3.1e-05, 'epoch': 6.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 320/400 [04:26<01:06,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0037, 'learning_rate': 3.2000000000000005e-05, 'epoch': 6.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 330/400 [04:34<00:57,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0267, 'learning_rate': 3.3e-05, 'epoch': 6.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 340/400 [04:43<00:50,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0792, 'learning_rate': 3.4000000000000007e-05, 'epoch': 6.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 350/400 [04:51<00:41,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0317, 'learning_rate': 3.5e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 360/400 [04:59<00:33,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.188, 'learning_rate': 3.6e-05, 'epoch': 7.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 370/400 [05:08<00:25,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0374, 'learning_rate': 3.7e-05, 'epoch': 7.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 380/400 [05:16<00:16,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0149, 'learning_rate': 3.8e-05, 'epoch': 7.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 390/400 [05:25<00:08,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0334, 'learning_rate': 3.9000000000000006e-05, 'epoch': 7.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [05:33<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0956, 'learning_rate': 4e-05, 'epoch': 8.0}\n",
      "{'train_runtime': 333.5862, 'train_samples_per_second': 19.185, 'train_steps_per_second': 1.199, 'train_loss': 0.27173578286543487, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.27173578286543487, metrics={'train_runtime': 333.5862, 'train_samples_per_second': 19.185, 'train_steps_per_second': 1.199, 'train_loss': 0.27173578286543487, 'epoch': 8.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Trainer.train을 사용한 파인튜닝\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'positive', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# 파인튜닝 이후 결과 판별\n",
    "texts = [\n",
    "    \"I feel fantastic\",\n",
    "    \"My life is going something wrong\",\n",
    "    \"I have not figured out what the chosen title has to do with the movie.\"\n",
    "]\n",
    "\n",
    "input_tokens = tokenizer(texts, truncation=True, padding=True)\n",
    "outputs = model(torch.tensor(input_tokens['input_ids']).to(device))\n",
    "label_dict = {0:'positive', 1:'negative'}\n",
    "\n",
    "print([label_dict[i] for i in torch.argmax(outputs['logits'], axis=1).cpu().numpy()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch를 이용한 fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "# 1) loading pre training model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
