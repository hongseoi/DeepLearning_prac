{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 이변수 함수\n",
    "$$\n",
    "f(x,y)=xy\n",
    "$$\n",
    "에 대하여 learning rate $\\eta=1$과 momentum 계수 $\\alpha=1$로 Momentum을 적용하려 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i) 초기 위치 ${\\bf x}_0=(1,2)$에서 출발하여 세 발자국 걸어갈때, ${\\bf x}_1$, ${\\bf x}_2$, ${\\bf x}_3$를 구하시오.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편미분하여 그레디언트\n",
    "$$\n",
    "\\nabla f (x,y) = (y,x)\n",
    "$$\n",
    "를 구합니다.  \n",
    "출발점에서는 관성이 없기 때문에 경사하강법과 동일합니다.  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf v_0} &= - \\eta \\nabla f({\\bf x_0}) = (-2,-1) \\\\\n",
    "{\\bf x_1} &= {\\bf x_0} + {\\bf v_0} = (1,2) + (-2,-1) = (-1,1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "두번째부터는 관성도 고려해야 합니다.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf v_1} &= \\alpha {\\bf v_0} - \\eta \\nabla f({\\bf x_1}) = (-2,-1) - (1,-1) = (-3,0)\\\\\n",
    "{\\bf x_2} &= {\\bf x_1} + {\\bf v_1} = (-1,1) + (-3,0) = (-4,1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "세번째 걸음입니다.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf v_2} &= \\alpha {\\bf v_1} - \\eta \\nabla f({\\bf x_2}) = (-3,0) - (1,-4) = (-4,4)\\\\\n",
    "{\\bf x_3} &= {\\bf x_2} + {\\bf v_2} = (-4,1) + (-4,4) = (-8,5)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(ii) 코드로 검산하시오**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': -1, 'y': 1}\n",
      "{'x': -4, 'y': 1}\n",
      "{'x': -8, 'y': 5}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C://Users//HAN//Documents//Deep Learning from Scratch') # 각자의 경로로 수정해주세요.\n",
    "import numpy as np\n",
    "from common.optimizer import Momentum\n",
    "\n",
    "optimizer = Momentum(1,1)\n",
    "\n",
    "params = {'x':1,'y':2}\n",
    "\n",
    "def df():\n",
    "    return {'x': params['y'], 'y' : params['x']}\n",
    "\n",
    "for step in range(3):\n",
    "    grads = df()\n",
    "    optimizer.update(params,grads)\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. 이변수 함수\n",
    "$$\n",
    "f(x,y)=x^2 + xy\n",
    "$$\n",
    "에 대하여 learning rate $\\eta=1$과 momentum 계수 $\\alpha=1$로 Momentum을 적용하려 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i) 초기 위치 ${\\bf x}_0=(1,1)$에서 출발하여 세 발자국 걸어갈때, ${\\bf x}_1$, ${\\bf x}_2$, ${\\bf x}_3$를 구하시오.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편미분하여 그레디언트\n",
    "$$\n",
    "\\nabla f (x,y) = (2x+y,x)\n",
    "$$\n",
    "를 구합니다.  \n",
    "출발점에서는 관성이 없기 때문에 경사하강법과 동일합니다.  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf v_0} &= - \\eta \\nabla f({\\bf x_0}) = (-3,-1) \\\\\n",
    "{\\bf x_1} &= {\\bf x_0} + {\\bf v_0} = (1,1) + (-3,-1) = (-2,0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "두번째부터는 관성도 고려해야 합니다.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf v_1} &= \\alpha {\\bf v_0} - \\eta \\nabla f({\\bf x_1}) = (-3,-1) - (-4,-2) = (1,1)\\\\\n",
    "{\\bf x_2} &= {\\bf x_1} + {\\bf v_1} = (-2,0) + (1,1) = (-1,1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "세번째 걸음입니다.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf v_2} &= \\alpha {\\bf v_1} - \\eta \\nabla f({\\bf x_2}) = (1,1) - (-1,-1) = (2,2)\\\\\n",
    "{\\bf x_3} &= {\\bf x_2} + {\\bf v_2} = (-1,1) + (2,2) = (1,3)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(ii) 코드로 검산하시오**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': -2, 'y': 0}\n",
      "{'x': -1, 'y': 1}\n",
      "{'x': 1, 'y': 3}\n"
     ]
    }
   ],
   "source": [
    "optimizer = Momentum(1,1)\n",
    "\n",
    "params = {'x':1,'y':1}\n",
    "\n",
    "def df():\n",
    "    return {'x': 2*params['x']+params['y'], 'y' : params['x']}\n",
    "\n",
    "for step in range(3):\n",
    "    grads = df()\n",
    "    optimizer.update(params,grads)\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. 삼변수 함수\n",
    "$$\n",
    "f(x,y,z)=xyz\n",
    "$$\n",
    "에 대하여 learning rate $\\eta=1$과 관성 계수 $\\alpha=1$로 Momentum을 적용하려 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i) 초기 위치 ${\\bf x}_0=(1,2,1)$에서 출발하여 세 발자국 걸어갈 때, ${\\bf x}_1$, ${\\bf x}_2$, ${\\bf x}_3$를 구하시오.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편미분하여 그레디언트\n",
    "$$\n",
    "\\nabla f (x,y) = (yz,xz,xy)\n",
    "$$\n",
    "를 구합니다.  \n",
    "출발점에서는 관성이 없기 때문에 경사하강법과 동일합니다.  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf v_0} &= - \\eta \\nabla f({\\bf x_0}) = (-2,-1,-2) \\\\\n",
    "{\\bf x_1} &= {\\bf x_0} + {\\bf v_0} = (1,2,1) + (-2,-1,-2) = (-1,1,-1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "두번째부터는 관성도 고려해야 합니다.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf v_1} &= \\alpha {\\bf v_0} - \\eta \\nabla f({\\bf x_1}) = (-2,-1,-2) - (-1,1,-1) = (-1,-2,-1)\\\\\n",
    "{\\bf x_2} &= {\\bf x_1} + {\\bf v_1} = (-1,1,-1) + (-1,-2,-1) = (-2,-1,-2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "세번째 걸음입니다.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf v_2} &= \\alpha {\\bf v_1} - \\eta \\nabla f({\\bf x_2}) = (-1,-2,-1) - (2,4,2) = (-3,-6,-3)\\\\\n",
    "{\\bf x_3} &= {\\bf x_2} + {\\bf v_2} = (-2,-1,-2) + (-3,-6,-3) = (-5,-7,-5)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(ii) 코드로 검산하시오**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': -1, 'y': 1, 'z': -1}\n",
      "{'x': -2, 'y': -1, 'z': -2}\n",
      "{'x': -5, 'y': -7, 'z': -5}\n"
     ]
    }
   ],
   "source": [
    "optimizer = Momentum(1,1)\n",
    "\n",
    "params = {'x':1,'y':2,'z':1}\n",
    "\n",
    "def df():\n",
    "    return {'x': params['y']*params['z'], 'y' : params['x']*params['z'], 'z': params['x']*params['y']}\n",
    "\n",
    "for step in range(3):\n",
    "    grads = df()\n",
    "    optimizer.update(params,grads)\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 이변수 함수\n",
    "$$\n",
    "f(x,y)=xy\n",
    "$$\n",
    "에 대하여 learning rate $\\eta=1$과 momentum 계수 $\\alpha=1$로 NAG를 적용하려 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i) 초기 위치 ${\\bf x}_0=(1,2)$에서 출발하여 세 발자국 걸어갈때, ${\\bf x}_1$, ${\\bf x}_2$, ${\\bf x}_3$를 구하시오.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편미분하여 그레디언트\n",
    "$$\n",
    "\\nabla f (x,y) = (y,x)\n",
    "$$\n",
    "를 구합니다.  \n",
    "출발점에서는 관성이 없기 때문에 경사하강법과 동일합니다.  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf v_0} &= - \\eta \\nabla f({\\bf x_0}) = (-2,-1) \\\\\n",
    "{\\bf x_1} &= {\\bf x_0} + {\\bf v_0} = (1,2) + (-2,-1) = (-1,1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "두번째부터는 관성도 고려해야 합니다.  \n",
    "NAG가 Momentum과 다른 점은 모멘텀 스텝만큼 미리 간 지점 $\\bf x_1'$에서 그레디언트를 구한다는 것입니다.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf x_1'} &= {\\bf x_1} + \\alpha {\\bf v_0} = (-1,1)+(-2,-1) = (-3,0)\\\\\n",
    "{\\bf v_1} &= \\alpha {\\bf v_0} - \\eta \\nabla f({\\bf x_1'}) = (-2,-1) - (0,-3) = (-2,2)\\\\\n",
    "{\\bf x_2} &= {\\bf x_1} + {\\bf v_1} = (-1,1) + (-2,2) = (-3,3)\n",
    "\\end{aligned}\n",
    "$$\n",
    "세번째 걸음도 모멘텀 스텝만큼 미리 간 지점 $\\bf x_2'$에서 그레디언트를 구합니다.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf x_2'} &= {\\bf x_2} + \\alpha {\\bf v_1} = (-3,3)+(-2,2) = (-5,5)\\\\\n",
    "{\\bf v_2} &= \\alpha {\\bf v_1} - \\eta \\nabla f({\\bf x_2}) = (-2,2) - (5,-5) = (-7,7)\\\\\n",
    "{\\bf x_3} &= {\\bf x_2} + {\\bf v_2} = (-3,3) + (-7,7) = (-10,10)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(ii) 코드로 모멘텀 스텝만큼 미리 간 지점 ${\\bf x'}_1$, ${\\bf x'}_2$를 검산하시오.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "교재 Nesterov코드에서 self.v와 params의 실행 순서를 바꿔야 제대로 작동합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nesterov:\n",
    "  \n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            params[key] += self.momentum * self.momentum * self.v[key]\n",
    "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
    "            self.v[key] *= self.momentum\n",
    "            self.v[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': -3, 'y': 0}\n",
      "{'x': -5, 'y': 5}\n"
     ]
    }
   ],
   "source": [
    "optimizer = Nesterov(1,1)\n",
    "\n",
    "params = {'x':1,'y':2}\n",
    "\n",
    "def df():\n",
    "    return {'x': params['y'], 'y' : params['x']}\n",
    "\n",
    "for step in range(2):\n",
    "    grads = df()\n",
    "    optimizer.update(params,grads)\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. 이변수 함수\n",
    "$$\n",
    "f(x,y)=x^2 + xy\n",
    "$$\n",
    "에 대하여 learning rate $\\eta=1$과 momentum 계수 $\\alpha=1$로 NAG를 적용하려 한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i) 초기 위치 ${\\bf x}_0=(1,1)$에서 출발하여 세 발자국 걸어갈때, ${\\bf x}_1$, ${\\bf x}_2$, ${\\bf x}_3$를 구하시오.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편미분하여 그레디언트\n",
    "$$\n",
    "\\nabla f (x,y) = (2x+y,x)\n",
    "$$\n",
    "를 구합니다.  \n",
    "출발점에서는 관성이 없기 때문에 경사하강법과 동일합니다.  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf v_0} &= - \\eta \\nabla f({\\bf x_0}) = (-3,-1) \\\\\n",
    "{\\bf x_1} &= {\\bf x_0} + {\\bf v_0} = (1,1) + (-3,-1) = (-2,0)\n",
    "\\end{aligned}\n",
    "$$\n",
    "두번째부터는 관성도 고려해야 합니다.  \n",
    "NAG가 Momentum과 다른 점은 모멘텀 스텝만큼 미리 간 지점 $\\bf x_1'$에서 그레디언트를 구한다는 것입니다.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf x_1'} &= {\\bf x_1} + \\alpha {\\bf v_0} = (-2,0)+(-3,-1) = (-5,-1)\\\\\n",
    "{\\bf v_1} &= \\alpha {\\bf v_0} - \\eta \\nabla f({\\bf x_1'}) = (-3,-1) - (-11,-5) = (8,4)\\\\\n",
    "{\\bf x_2} &= {\\bf x_1} + {\\bf v_1} = (-2,0) + (8,4) = (6,4)\n",
    "\\end{aligned}\n",
    "$$\n",
    "세번째 걸음도 모멘텀 스텝만큼 미리 간 지점 $\\bf x_2'$에서 그레디언트를 구합니다.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf x_2'} &= {\\bf x_2} + \\alpha {\\bf v_1} = (6,4)+(8,4) = (14,8)\\\\\n",
    "{\\bf v_2} &= \\alpha {\\bf v_1} - \\eta \\nabla f({\\bf x_2}) = (8,4) - (36,14) = (-28,-10)\\\\\n",
    "{\\bf x_3} &= {\\bf x_2} + {\\bf v_2} = (6,4) + (-28,-10) = (-22,-6)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(ii) 코드로 모멘텀 스텝만큼 미리 간 지점 ${\\bf x'}_1$, ${\\bf x'}_2$를 검산하시오.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': -5, 'y': -1}\n",
      "{'x': 14, 'y': 8}\n"
     ]
    }
   ],
   "source": [
    "optimizer = Nesterov(1,1)\n",
    "\n",
    "params = {'x':1,'y':1}\n",
    "\n",
    "def df():\n",
    "    return {'x': 2*params['x']+params['y'], 'y' : params['x']}\n",
    "\n",
    "for step in range(2):\n",
    "    grads = df()\n",
    "    optimizer.update(params,grads)\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. NAG(Nesterov Accelated Gradient) 점화식은\n",
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf v}_n &= \\alpha {\\bf v}_{n-1} - \\eta \\nabla f({\\bf x}_n+\\alpha {\\bf v}_{n-1}) \\\\\n",
    "{\\bf x}_{n+1} &= {\\bf x}_n + {\\bf v}_n\n",
    "\\end{aligned}\n",
    "$$\n",
    "이다.\n",
    "현재 위치 ${\\bf x}_n$에서 모멘텀 스텝 후의 위치를\n",
    "$$\n",
    "{\\bf x}_n'={\\bf x}_n+\\alpha {\\bf v}_{n-1}\n",
    "$$\n",
    "이라 하자.\n",
    "점화식\n",
    "$$\n",
    "{\\bf x}_{n+1}' ={\\bf x}_n' - (1+\\alpha) \\eta \\nabla f({\\bf x}_n') + \\alpha^2 {\\bf v}_{n-1}\n",
    "$$\n",
    "을 유도하시오.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "{\\bf x}_{n+1}' & = {\\bf x}_{n+1} + \\alpha {\\bf v}_n \\\\\n",
    "& = ({\\bf x}_n + {\\bf v}_n) + \\alpha {\\bf v}_n \\\\\n",
    "& = {\\bf x}_n + (1 + \\alpha) {\\bf v}_n \\\\\n",
    "& = ({\\bf x}_n' - \\alpha {\\bf v}_{n-1}) + (1 + \\alpha) (\\alpha {\\bf v}_{n-1} - \\eta \\nabla f({\\bf x}_n')) \\\\\n",
    "& = {\\bf x}_n' - (1+\\alpha) \\eta \\nabla f({\\bf x}_n') + \\alpha^2 {\\bf v}_{n-1} \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
