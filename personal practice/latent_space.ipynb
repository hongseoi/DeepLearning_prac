{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project running on device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary # 디버깅 쉽게 해줌\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Project running on device: \", DEVICE)\n",
    "\n",
    "# configurations for the task\n",
    "config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 3,\n",
    "    \"lr\": 5e-4,   # learning rate\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    process = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Pad([2])]\n",
    "                )\n",
    " \n",
    "    # x - images; we process each image in the batch\n",
    "    x = [process(data[0]) for data in batch]\n",
    "    x = torch.concat(x).unsqueeze(1)\n",
    "    \n",
    "    # y - labels, note that we should convert the labels to LongTensor\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"T-shirt/top\", \n",
    "          \"Trouser\", \n",
    "          \"Pullover\", \n",
    "          \"Dress\", \n",
    "          \"Coat\", \n",
    "          \"Sandla\", \n",
    "          \"Shirt\", \n",
    "          \"Sneaker\", \n",
    "          \"Bag\", \n",
    "          \"Ankle boot\"]\n",
    "\n",
    "# download/load dataset\n",
    "train_data = FashionMNIST(\"./MNIST_DATA\", train=True, download=True)\n",
    "valid_data = FashionMNIST(\"./MNIST_DATA\", train=False, download=True)\n",
    "\n",
    "# put datasets into dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], \n",
    "                          shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_data, batch_size=config[\"batch_size\"], \n",
    "                           shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting train data: \n",
      "Batch shape:  torch.Size([64, 1, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Inspecting train data: \")\n",
    "for _, data in enumerate(train_loader):\n",
    "    print(\"Batch shape: \", data[0].shape)\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(10, 4))\n",
    "\n",
    "    for i in range(4):\n",
    "        # Ture 3D tensor to 2D tensor due to image's single channel\n",
    "        ax[i].imshow(data[0][i].squeeze(), cmap=\"gray\")\n",
    "        ax[i].axis(\"off\")\n",
    "        ax[i].set_title(labels[data[1][i]])\n",
    "    plt.show()\n",
    "    # And don't forget to break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters:\n",
    "LAYERS = 3\n",
    "KERNELS = [3, 3, 3]\n",
    "CHANNELS = [32, 64, 128]\n",
    "STRIDES = [2, 2, 2]\n",
    "LINEAR_DIM = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, output_dim=2, use_batchnorm=False, use_dropout=False):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # bottleneck dimentionality\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # variables deciding if using dropout and batchnorm in model\n",
    "        self.use_dropout = use_dropout\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "\n",
    "        # convolutional layer hyper parameters\n",
    "        self.layers = LAYERS\n",
    "        self.kernels = KERNELS\n",
    "        self.channels = CHANNELS\n",
    "        self.strides = STRIDES\n",
    "        self.conv = self.get_convs()\n",
    "\n",
    "        # layers for latent space projection\n",
    "        self.fc_dim = LINEAR_DIM\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(self.fc_dim, self.output_dim)\n",
    "\n",
    "    def get_convs(self):\n",
    "        conv_layers = nn.Sequential()\n",
    "        for i in range(self.layers):\n",
    "            # input channel of the first layer is 1\n",
    "            if i == 0:\n",
    "                conv_layers.append(nn.Conv2d(\n",
    "                    1, \n",
    "                    self.channels[i],\n",
    "                    kernel_size=self.kernels[i],\n",
    "                    padding=1\n",
    "                ))\n",
    "            else:\n",
    "                conv_layers.append(nn.Conv2d(\n",
    "                    self.channels[i-1],\n",
    "                    self.channels[i],\n",
    "                    kernel_size=self.kernels[i],\n",
    "                    stride=self.strides[i],\n",
    "                    padding=1\n",
    "                ))\n",
    "            if self.use_batchnorm:\n",
    "                conv_layers.append(nn.BatchNorm2d(self.channels[i]))\n",
    "            \n",
    "            conv_layers.append(nn.GELU())\n",
    "\n",
    "            if self.use_dropout:\n",
    "                conv_layers.append(nn.Dropout2d(0.15))\n",
    "        \n",
    "        return conv_layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=2, use_batchnorm=False, use_dropout=False):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # variables deciding if using dropout and batchnorm in model\n",
    "        self.use_dropout = use_dropout\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "\n",
    "        self.fc_dim = LINEAR_DIM\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Conv layer hypyer parameters\n",
    "        self.layers = LAYERS\n",
    "        self.kernels = KERNELS\n",
    "        self.channels = CHANNELS[::-1] # flip the channel dimensions\n",
    "        self.strides = STRIDES\n",
    "        \n",
    "        # In decoder, we first do fc project, then conv layers\n",
    "        self.linear = nn.Linear(self.input_dim, self.fc_dim)\n",
    "        self.conv =  self.get_convs()\n",
    "\n",
    "        self.output = nn.Conv2d(self.channels[-1], 1, kernel_size=1, stride=1)\n",
    "\n",
    "    def get_convs(self):\n",
    "        conv_layers = nn.Sequential()\n",
    "        for i in range(self.layers):\n",
    "            \n",
    "            if i == 0: conv_layers.append(\n",
    "                            nn.ConvTranspose2d(self.channels[i],\n",
    "                                               self.channels[i],\n",
    "                                               kernel_size=self.kernels[i],\n",
    "                                               stride=self.strides[i],\n",
    "                                               padding=1,\n",
    "                                               output_padding=1)\n",
    "                            )\n",
    "            \n",
    "            else: conv_layers.append(\n",
    "                            nn.ConvTranspose2d(self.channels[i-1], \n",
    "                                               self.channels[i],\n",
    "                                               kernel_size=self.kernels[i],\n",
    "                                               stride=self.strides[i],\n",
    "                                               padding=1,\n",
    "                                               output_padding=1\n",
    "                                              )\n",
    "                            )\n",
    "            \n",
    "            if self.use_batchnorm and i != self.layers - 1:\n",
    "                conv_layers.append(nn.BatchNorm2d(self.channels[i]))\n",
    "\n",
    "            conv_layers.append(nn.GELU())\n",
    "\n",
    "            if self.use_dropout:\n",
    "                conv_layers.append(nn.Dropout2d(0.15))\n",
    "\n",
    "        return conv_layers\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # reshape 3D tensor to 4D tensor\n",
    "        x = x.reshape(x.shape[0], 128, 4, 4)\n",
    "        x = self.conv(x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(output_dim=2, \n",
    "                               use_batchnorm=True, use_dropout=False)\n",
    "        self.decoder = Decoder(input_dim=2,\n",
    "                               use_batchnorm=True, use_dropout=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(step:int=0, show=False):\n",
    "    \n",
    "    model.eval() # Switch the model to evaluation mode\n",
    "    \n",
    "    points = []\n",
    "    label_idcs = []\n",
    "    \n",
    "    path = \"./ScatterPlots\"\n",
    "    if not os.path.exists(path): os.mkdir(path)\n",
    "    \n",
    "    for i, data in enumerate(valid_loader):\n",
    "        img, label = [d.to(DEVICE) for d in data]\n",
    "        # We only need to encode the validation images\n",
    "        proj = model.encoder(img)\n",
    "        points.extend(proj.detach().cpu().numpy())\n",
    "        label_idcs.extend(label.detach().cpu().numpy())\n",
    "        del img, label\n",
    "    \n",
    "    points = np.array(points)\n",
    "    \n",
    "    # Creating a scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10) if not show else (8, 8))\n",
    "    scatter = ax.scatter(x=points[:, 0], y=points[:, 1], s=2.0, \n",
    "                c=label_idcs, cmap='tab10', alpha=0.9, zorder=2)\n",
    "    \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    \n",
    "    if show: \n",
    "        ax.grid(True, color=\"lightgray\", alpha=1.0, zorder=0)\n",
    "        plt.show()\n",
    "    else: \n",
    "        # Do not show but only save the plot in training\n",
    "        plt.savefig(f\"{path}/Step_{step:03d}.png\", bbox_inches=\"tight\")\n",
    "        plt.close() # don't forget to close the plot, or it is always in memory\n",
    "        model.train()\n",
    "\n",
    "# convert image sequence to a gif file\n",
    "def save_gif():\n",
    "  \n",
    "  frames = []\n",
    "  imgs = sorted(os.listdir(\"./ScatterPlots\"))\n",
    "\n",
    "  for im in imgs:\n",
    "      new_frame = Image.open(\"./ScatterPlots/\" + im)\n",
    "      frames.append(new_frame)\n",
    "  \n",
    "  frames[0].save(\"latentspace.gif\", format=\"GIF\",\n",
    "                 append_images=frames[1:],\n",
    "                 save_all=True,\n",
    "                 duration=200, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "model=AutoEncoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=1e-5)\n",
    "\n",
    "# For mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "steps = 0 # tracking the training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, save_distrib=False):\n",
    "    # steps is used to track training progress, purely for latent space plots\n",
    "    global steps \n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Process tqdm bar, helpful for monitoring training process\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, \n",
    "                     leave=False, position=0, desc=\"Train\")\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x = batch[0].to(DEVICE)\n",
    "        \n",
    "        # Here we implement the mixed precision training\n",
    "        with torch.cuda.amp.autocast():\n",
    "            y_recons = model(x)\n",
    "            loss = criterion(y_recons, x)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        batch_bar.set_postfix(\n",
    "            loss=f\"{train_loss/(i+1):.4f}\",\n",
    "            lr = f\"{optimizer.param_groups[0]['lr']:.4f}\"\n",
    "        )\n",
    "        batch_bar.update()        \n",
    "\n",
    "        # Saving latent space plots\n",
    "        if steps % 10 == 0 and save_distrib and steps <= 400: plotting(steps)\n",
    "        steps += 1        \n",
    "        \n",
    "        # remove unnecessary cache in CUDA memory\n",
    "        torch.cuda.empty_cache()\n",
    "        del x, y_recons\n",
    "    \n",
    "    batch_bar.close()\n",
    "    train_loss /= len(dataloader)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    \n",
    "    model.eval() # Don't forget to turn the model to eval mode\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # Progress tqdm bar\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True,\n",
    "                     leave=False, position=0, desc=\"Validation\")\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x = batch[0].to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad(): # we don't need gradients in validation\n",
    "            y_recons = model(x)\n",
    "            loss = criterion(y_recons, x)\n",
    "        \n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "        batch_bar.set_postfix(\n",
    "            loss=f\"{valid_loss/(i+1):.4f}\",\n",
    "            lr = f\"{optimizer.param_groups[0]['lr']:.4f}\"\n",
    "        )\n",
    "        batch_bar.update()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        del x, y_recons\n",
    "    \n",
    "    batch_bar.close()\n",
    "    valid_loss /= len(dataloader)\n",
    "    \n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9651e13bbaff4838a91eb03b22001f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (struct c10::Half) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m      3\u001b[0m     curr_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(model, train_loader, criterion, \n\u001b[0;32m      5\u001b[0m                        optimizer, save_distrib\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m validate(model, valid_loader, criterion)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mlr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_lr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, criterion, optimizer, save_distrib)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Here we implement the mixed precision training\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[1;32m---> 19\u001b[0m     y_recons \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_recons, x)\n\u001b[0;32m     22\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mAutoEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x))\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 54\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[0;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (struct c10::Half) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "for i in range(config[\"epochs\"]):\n",
    "\n",
    "    curr_lr = float(optimizer.param_groups[0][\"lr\"])\n",
    "    train_loss = train(model, train_loader, criterion, \n",
    "                       optimizer, save_distrib=True)\n",
    "    valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {i+1}/{config['epochs']}\\nTrain loss: {train_loss:.4f}\\t Validation loss: {valid_loss:.4f}\\tlr: {curr_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
