{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project running on device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# torchsummary for easy checking and debugging\n",
    "from torchsummary import summary\n",
    "\n",
    "# torchvision for downloading and processing dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "# others for notebook UI and latent space visualization\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Checking is CUDA available on current machine\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Project running on device: \", DEVICE)\n",
    "\n",
    "# configurations for the task\n",
    "config = {\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 1000,\n",
    "    \"lr\": 5e-4,   # learning rate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function convert the PIL images to tensors then pad them\n",
    "def collate_fn(batch):\n",
    "    process = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Pad([2])]\n",
    "                )\n",
    " \n",
    "    # x - images; we process each image in the batch\n",
    "    x = [process(data[0]) for data in batch]\n",
    "    x = torch.concat(x).unsqueeze(1)\n",
    "    \n",
    "    # y - labels, note that we should convert the labels to LongTensor\n",
    "    y = torch.LongTensor([data[1] for data in batch])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"T-shirt/top\", \n",
    "          \"Trouser\", \n",
    "          \"Pullover\", \n",
    "          \"Dress\", \n",
    "          \"Coat\", \n",
    "          \"Sandla\", \n",
    "          \"Shirt\", \n",
    "          \"Sneaker\", \n",
    "          \"Bag\", \n",
    "          \"Ankle boot\"]\n",
    "\n",
    "# download/load dataset\n",
    "train_data = FashionMNIST(\"./MNIST_DATA\", train=True, download=True)\n",
    "valid_data = FashionMNIST(\"./MNIST_DATA\", train=False, download=True)\n",
    "\n",
    "# put datasets into dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], \n",
    "                          shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_data, batch_size=config[\"batch_size\"], \n",
    "                           shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting train data: \n",
      "Batch shape:  torch.Size([64, 1, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Inspecting train data: \")\n",
    "for _, data in enumerate(train_loader):\n",
    "    print(\"Batch shape: \", data[0].shape)\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(10, 4))\n",
    "\n",
    "    for i in range(4):\n",
    "        # Ture 3D tensor to 2D tensor due to image's single channel\n",
    "        ax[i].imshow(data[0][i].squeeze(), cmap=\"gray\")\n",
    "        ax[i].axis(\"off\")\n",
    "        ax[i].set_title(labels[data[1][i]])\n",
    "    plt.show()\n",
    "    # And don't forget to break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters:\n",
    "LAYERS = 3\n",
    "KERNELS = [3, 3, 3]\n",
    "CHANNELS = [32, 64, 128]\n",
    "STRIDES = [2, 2, 2]\n",
    "LINEAR_DIM = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_dim=2, use_batchnorm=False, use_dropout=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # bottleneck dimentionality\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # variables deciding if using dropout and batchnorm in model\n",
    "        self.use_dropout = use_dropout\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "    \n",
    "        # convolutional layer hyper parameters\n",
    "        self.layers = LAYERS\n",
    "        self.kernels = KERNELS\n",
    "        self.channels = CHANNELS\n",
    "        self.strides = STRIDES\n",
    "        self.conv = self.get_convs()\n",
    "        \n",
    "        # layers for latent space projection\n",
    "        self.fc_dim = LINEAR_DIM\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(self.fc_dim, self.output_dim)\n",
    "    \n",
    "    \n",
    "    def get_convs(self):\n",
    "        \"\"\"\n",
    "        generating convolutional layers based on model's \n",
    "        hyper parameters\n",
    "        \"\"\"\n",
    "        conv_layers = nn.Sequential()\n",
    "        for i in range(self.layers):\n",
    "            # The input channel of the first layer is 1\n",
    "            if i == 0: conv_layers.append(nn.Conv2d(1, \n",
    "                                              self.channels[i], \n",
    "                                              kernel_size=self.kernels[i],\n",
    "                                              stride=self.strides[i],\n",
    "                                              padding=1))\n",
    "            \n",
    "            else: conv_layers.append(nn.Conv2d(self.channels[i-1], \n",
    "                                         self.channels[i],\n",
    "                                         kernel_size=self.kernels[i],\n",
    "                                         stride=self.strides[i],\n",
    "                                         padding=1))\n",
    "            \n",
    "            if self.use_batchnorm:\n",
    "                conv_layers.append(nn.BatchNorm2d(self.channels[i]))\n",
    "            \n",
    "            # Here we use GELU as activation function\n",
    "            conv_layers.append(nn.GELU()) \n",
    "            \n",
    "            if self.use_dropout:\n",
    "                conv_layers.append(nn.Dropout2d(0.15))\n",
    "\n",
    "        return conv_layers\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=2, use_batchnorm=False, use_dropout=False):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # variables deciding if using dropout and batchnorm in model\n",
    "        self.use_dropout = use_dropout\n",
    "        self.use_batchnorm = use_batchnorm\n",
    "\n",
    "        self.fc_dim = LINEAR_DIM\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Conv layer hypyer parameters\n",
    "        self.layers = LAYERS\n",
    "        self.kernels = KERNELS\n",
    "        self.channels = CHANNELS[::-1] # flip the channel dimensions\n",
    "        self.strides = STRIDES\n",
    "        \n",
    "        # In decoder, we first do fc project, then conv layers\n",
    "        self.linear = nn.Linear(self.input_dim, self.fc_dim)\n",
    "        self.conv =  self.get_convs()\n",
    "\n",
    "        self.output = nn.Conv2d(self.channels[-1], 1, kernel_size=1, stride=1)\n",
    "\n",
    "    def get_convs(self):\n",
    "        conv_layers = nn.Sequential()\n",
    "        for i in range(self.layers):\n",
    "            \n",
    "            if i == 0: conv_layers.append(\n",
    "                            nn.ConvTranspose2d(self.channels[i],\n",
    "                                               self.channels[i],\n",
    "                                               kernel_size=self.kernels[i],\n",
    "                                               stride=self.strides[i],\n",
    "                                               padding=1,\n",
    "                                               output_padding=1)\n",
    "                            )\n",
    "            \n",
    "            else: conv_layers.append(\n",
    "                            nn.ConvTranspose2d(self.channels[i-1], \n",
    "                                               self.channels[i],\n",
    "                                               kernel_size=self.kernels[i],\n",
    "                                               stride=self.strides[i],\n",
    "                                               padding=1,\n",
    "                                               output_padding=1\n",
    "                                              )\n",
    "                            )\n",
    "            \n",
    "            if self.use_batchnorm and i != self.layers - 1:\n",
    "                conv_layers.append(nn.BatchNorm2d(self.channels[i]))\n",
    "\n",
    "            conv_layers.append(nn.GELU())\n",
    "\n",
    "            if self.use_dropout:\n",
    "                conv_layers.append(nn.Dropout2d(0.15))\n",
    "\n",
    "        return conv_layers\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        # reshape 3D tensor to 4D tensor\n",
    "        x = x.reshape(x.shape[0], 128, 4, 4)\n",
    "        x = self.conv(x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = Encoder(output_dim=2, \n",
    "                               use_batchnorm=True, use_dropout=False)\n",
    "        self.decoder = Decoder(input_dim=2,\n",
    "                               use_batchnorm=True, use_dropout=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(step:int=0, show=False):\n",
    "    \n",
    "    model.eval() # Switch the model to evaluation mode\n",
    "    \n",
    "    points = []\n",
    "    label_idcs = []\n",
    "    \n",
    "    path = \"./ScatterPlots\"\n",
    "    if not os.path.exists(path): os.mkdir(path)\n",
    "    \n",
    "    for i, data in enumerate(valid_loader):\n",
    "        img, label = [d.to(DEVICE) for d in data]\n",
    "        # We only need to encode the validation images\n",
    "        proj = model.encoder(img) # encoder(img)에서 수정\n",
    "        points.extend(proj.detach().cpu().numpy())\n",
    "        label_idcs.extend(label.detach().cpu().numpy())\n",
    "        del img, label\n",
    "    \n",
    "    points = np.array(points)\n",
    "    \n",
    "    # Creating a scatter plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 10) if not show else (8, 8))\n",
    "    scatter = ax.scatter(x=points[:, 0], y=points[:, 1], s=2.0, \n",
    "                c=label_idcs, cmap='tab10', alpha=0.9, zorder=2)\n",
    "    \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    \n",
    "    if show: \n",
    "        ax.grid(True, color=\"lightgray\", alpha=1.0, zorder=0)\n",
    "        plt.show()\n",
    "    else: \n",
    "        # Do not show but only save the plot in training\n",
    "        plt.savefig(f\"{path}/Step_{step:03d}.png\", bbox_inches=\"tight\")\n",
    "        plt.close() # don't forget to close the plot, or it is always in memory\n",
    "        model.train()\n",
    "\n",
    "# convert image sequence to a gif file\n",
    "def save_gif():\n",
    "  \n",
    "  frames = []\n",
    "  imgs = sorted(os.listdir(\"./ScatterPlots\"))\n",
    "\n",
    "  for im in imgs:\n",
    "      new_frame = Image.open(\"./ScatterPlots/\" + im)\n",
    "      frames.append(new_frame)\n",
    "  \n",
    "  frames[0].save(\"latentspace.gif\", format=\"GIF\",\n",
    "                 append_images=frames[1:],\n",
    "                 save_all=True,\n",
    "                 duration=200, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "model=AutoEncoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=1e-5)\n",
    "\n",
    "# For mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "steps = 0 # tracking the training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, save_distrib=False):\n",
    "    # steps is used to track training progress, purely for latent space plots\n",
    "    global steps \n",
    "    \n",
    "    model.to(DEVICE)  # 모델을 GPU로 이동\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    # Process tqdm bar, helpful for monitoring training process\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, \n",
    "                     leave=False, position=0, desc=\"Train\")\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x = batch[0].to(DEVICE)\n",
    "        x = x.float()\n",
    "        # Here we implement the mixed precision training\n",
    "        y_recons = model(x)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss = criterion(y_recons, x)\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        batch_bar.set_postfix(\n",
    "            loss=f\"{train_loss/(i+1):.4f}\",\n",
    "            lr = f\"{optimizer.param_groups[0]['lr']:.4f}\"\n",
    "        )\n",
    "        batch_bar.update()        \n",
    "\n",
    "        # Saving latent space plots\n",
    "        if steps % 10 == 0 and save_distrib: plotting(steps) # and steps <= 400:\n",
    "        steps += 1        \n",
    "        \n",
    "        # remove unnecessary cache in CUDA memory\n",
    "        torch.cuda.empty_cache()\n",
    "        del x, y_recons\n",
    "    \n",
    "    batch_bar.close()\n",
    "    train_loss /= len(dataloader)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion):\n",
    "    \n",
    "    model.eval() # Don't forget to turn the model to eval mode\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # Progress tqdm bar\n",
    "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True,\n",
    "                     leave=False, position=0, desc=\"Validation\")\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        x = batch[0].to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad(): # we don't need gradients in validation\n",
    "            y_recons = model(x)\n",
    "            loss = criterion(y_recons, x)\n",
    "        \n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "        batch_bar.set_postfix(\n",
    "            loss=f\"{valid_loss/(i+1):.4f}\",\n",
    "            lr = f\"{optimizer.param_groups[0]['lr']:.4f}\"\n",
    "        )\n",
    "        batch_bar.update()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        del x, y_recons\n",
    "    \n",
    "    batch_bar.close()\n",
    "    valid_loss /= len(dataloader)\n",
    "    \n",
    "    return valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bd2666662f47d9b5f63cb2a4ad9997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1e02f2bc8f44a097cb469e0aaa7495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "Train loss: 0.0282\t Validation loss: 0.0245\tlr: 0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f3593e4a784273a68923db1b41b1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9ceff0f1e64e78be61368ea5fd83a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000\n",
      "Train loss: 0.0242\t Validation loss: 0.0239\tlr: 0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e10d15fbf14b7a9c5ebd9211f02257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da0ca72827d47b6886d2041b325cef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000\n",
      "Train loss: 0.0234\t Validation loss: 0.0230\tlr: 0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9afc86e66544658e55b7ed8a9e8648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86292b120a15401f9d17ffb0634bba88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000\n",
      "Train loss: 0.0230\t Validation loss: 0.0226\tlr: 0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00871a665bcc473ab158ff3849f3e04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f753be7b844d59a4a83fd1ab87f616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000\n",
      "Train loss: 0.0226\t Validation loss: 0.0223\tlr: 0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843904b74db745c08b5488eb373d0e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d733ae684d3455aa09b8797b6e98625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000\n",
      "Train loss: 0.0223\t Validation loss: 0.0220\tlr: 0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739e5643d1b9471c8ad6684d703be973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9ad6564d0c42c7afb5e537084c8799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000\n",
      "Train loss: 0.0220\t Validation loss: 0.0220\tlr: 0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fc18632e6045c4a03662649a8a254a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bbbbb96c5d4a6fa9746ae75d462b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000\n",
      "Train loss: 0.0219\t Validation loss: 0.0218\tlr: 0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f9b47138794185907ceaa163e296e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a55dccd04e42c8a3a4f7f6b9d8baa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000\n",
      "Train loss: 0.0217\t Validation loss: 0.0216\tlr: 0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464af971a442452f8e06e35f986da979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb0378e04b9f4bce9ff86c96e283b921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000\n",
      "Train loss: 0.0215\t Validation loss: 0.0215\tlr: 0.0005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821386bf4cfa4964846edde1a1de0b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m      3\u001b[0m     curr_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(model, train_loader, criterion, \n\u001b[0;32m      5\u001b[0m                        optimizer, save_distrib\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m validate(model, valid_loader, criterion)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mlr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_lr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, criterion, optimizer, save_distrib)\u001b[0m\n\u001b[0;32m     23\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     25\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 26\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m     27\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     29\u001b[0m batch_bar\u001b[38;5;241m.\u001b[39mset_postfix(\n\u001b[0;32m     30\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m/\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m     lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:452\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    450\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 452\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    454\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:349\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\kaggle\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:349\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    343\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    348\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    350\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(config[\"epochs\"]):\n",
    "\n",
    "    curr_lr = float(optimizer.param_groups[0][\"lr\"])\n",
    "    train_loss = train(model, train_loader, criterion, \n",
    "                       optimizer, save_distrib=True)\n",
    "    valid_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "    print(f\"Epoch {i+1}/{config['epochs']}\\nTrain loss: {train_loss:.4f}\\t Validation loss: {valid_loss:.4f}\\tlr: {curr_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import os\n",
    "\n",
    "# PNG 파일이 있는 폴더 경로\n",
    "folder_path = r\"C:\\Users\\User\\Desktop\\스터디\\DeepLearning_prac\\personal practice\\ScatterPlots\"\n",
    "\n",
    "# 폴더 내 PNG 파일 목록 추출\n",
    "filenames = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".png\"):\n",
    "        filenames.append(os.path.join(folder_path, filename))\n",
    "\n",
    "# 이미지 읽기\n",
    "images = []\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread(filename))\n",
    "\n",
    "# GIF 저장\n",
    "imageio.imsave('my_animation.gif', images, format='GIF', duration=0.2)  # duration: 프레임 간격\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
