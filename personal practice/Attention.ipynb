{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type of Attention mechanism\n",
    "- ScaledDotProductAttention\n",
    "- DotProductAttention\n",
    "- AddictiveAttention\n",
    "- LocationawareAttention\n",
    "- MultiheadLocationawareAttention\n",
    "- MultiheadAttention\n",
    "- RelativeMultiheadAttention\n",
    "- CustomizingAttention\n",
    "- CrossAttention\n",
    "- GlobalAttention\n",
    "- HardAttention\n",
    "- SoftAttention\n",
    "- HierarchicalAttention\n",
    "- LocalAttention\n",
    "- MaskedAttention\n",
    "- SelfAttention\n",
    "- CasualSelfAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ScaledDotProductAttention\n",
    "- Attention is all you need 논문에서 제시\n",
    "- query와 key의 dot product 계산 후 sqrt(attention dim)으로 나누고, softmax 적용\n",
    "\n",
    "### Inputs: query, key, value, mask\n",
    "- **query** (batch, q_len, d_model): tensor containing projection vector for decoder.\n",
    "- **key** (batch, k_len, d_model): tensor containing projection vector for encoder.\n",
    "- **value** (batch, v_len, d_model): tensor containing features of the encoded input sequence.\n",
    "- **mask** (-): tensor containing indices to be masked\n",
    "\n",
    "### Returns: context, attn\n",
    "- **context**: tensor containing the context vector from attention mechanism.\n",
    "- **attn**: tensor containing the attention (alignment) from the encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casual Self Attention\n",
    "- self attention의 한 종류로, 주로 autoregressive 모델에서 사용됨\n",
    "- self attention에 시간적 제약 추가\n",
    "    - 특정 위치에서의 예측이 그 위치 이전의 정보에만 의존하도록 보장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads:int, embed_dimension:int, bias:bool=False, is_casual:bool=False, dropout:float=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dimension%num_heads == 0\n",
    "        \n",
    "        # qkv projection\n",
    "        # input: (batch_size, seq_len, embed_dimension)\n",
    "        # output: (batch_size, seq_len, 3 * embed_dimension)\n",
    "        self.c_attn = nn.Linear(embed_dimension, 3*embed_dimension, bias=bias) \n",
    "        self.c_proj = nn.Linear(embed_dimension, embed_dimension, bias=bias)\n",
    "        \n",
    "        # regularization\n",
    "        self.dropout = dropout\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dimension = embed_dimension\n",
    "        self.is_casual = is_casual\n",
    "        \n",
    "    def forward(self, x):\n",
    "        query_projected = self.c_attn(x) # (batch_size, seq_len, 3 * embed_dimension)\n",
    "        \n",
    "        batch_size = query_projected.size(0) # 0번째 차원의 크기\n",
    "        embed_dim = query_projected.size(2)\n",
    "        head_dim = embed_dim // (self.num_heads*3)\n",
    "        \n",
    "        query, key, value = query_projected.chunk(3, -1) # -1 축으로 3분할\n",
    "        \n",
    "        # tensor 형태 변경\n",
    "        query = query.view(batch_size, -1, self.num_heads, head_dim).transpose(1,2)\n",
    "        key = key.view(batch_size, -1, self.num_heads, head_dim).transpose(1,2)\n",
    "        value = value.view(batch_size, -1, self.num_heads, head_dim).transpose(1,2)\n",
    "        \n",
    "        if self.training:\n",
    "            dropout = self.dropout\n",
    "            is_casual = self.is_casual\n",
    "            \n",
    "        else:\n",
    "            dropout = 0.0\n",
    "            is_casual = False\n",
    "            \n",
    "        y = F.scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=dropout, is_causal=is_casual)\n",
    "        y = y.transpose(1,2).view(batch_size, -1, self.num_heads*head_dim)\n",
    "        \n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        \n",
    "        return y\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalSelfAttention(\n",
      "  (c_attn): Linear(in_features=512, out_features=1536, bias=False)\n",
      "  (c_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_heads = 8\n",
    "heads_per_dim = 64\n",
    "embed_dimension = num_heads * heads_per_dim\n",
    "dtype = torch.float16\n",
    "model = CausalSelfAttention(num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_casual=True, dropout=0.1).to(\"cuda\").to(dtype).eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
